{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPEvR7UiCGMTXLwWW+TMUol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cangokalp/char_rnn_rap_generator/blob/master/char_rnn_rap_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4m5mNc44dHC",
        "colab_type": "text"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/cangokalp/char_rnn_rap_generator/blob/master/char_rnn_rap_generator.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVi3DWY2OoSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "342cd285-78b8-4d87-fe44-33cac708bf15"
      },
      "source": [
        "import pdb\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !pip install -q -U tensorflow-addons\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 27.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 993kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGn2NDY3d74K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NOTE TO WILL: \n",
        "#Go to my github https://github.com/cangokalp/char_rnn_rap_generator\n",
        "#and download the two files; lyrics_array_mix.pickle and rap_bot_v2_model.h5, they are under data and saved model folders, respectively.\n",
        "#After that run the below cell. \n",
        "#Then on the tab to the left of the screen there is a folder icon, \n",
        "#click that and youll see data and saved_models folders. \n",
        "#click upload and upload the files you downloaded. \n",
        "#Then drag them in their corresponding folders\n",
        "#then you can continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okZ3iyatVLre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "e60dc858-330f-4f65-b862-d98284966bf4"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# DRIVE_PATH = '/content/gdrive/My Drive'\n",
        "\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "MODEL_PATH = os.path.join(PROJECT_ROOT_DIR, \"saved_models\")\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "\n",
        "def save(fname, data extension=\"pickle\"):\n",
        "    path = os.path.join(DATA_PATH, fname + \".\" + extension)\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "def load(fname, extension='pickle'):\n",
        "    path = os.path.join(DATA_PATH, fname + \".\" + extension)\n",
        "    \n",
        "    with open(path, 'rb') as f:\n",
        "        item = pickle.load(f)\n",
        "\n",
        "    return item"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-108-c75e94b5d19a>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    def save(fname, data extension=\"pickle\"):\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47e_XqDIbCuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GRAB TRAINING DATA:\n",
        "documents = load('lyrics_array_mix')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxtv1YLAO5LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CREATE NEW TRAINING DATA - IF YOU WANT\n",
        "\n",
        "import time\n",
        "!pip install lyricsgenius\n",
        "import lyricsgenius\n",
        "\n",
        "API_TOKEN = '1OWmIr37ZuUI5hDXjN_FIXJlbZadIqH6JBDObBCiF_MNySn4BQqh8aQ8OUYF5muk'\n",
        "fname = 'mix'\n",
        "\n",
        "#### grab the lyrics #####\n",
        "artists = []\n",
        "genius = lyricsgenius.Genius(API_TOKEN, skip_non_songs=True, remove_section_headers=True)\n",
        "\n",
        "lonely = genius.search_artist(\"The Lonely Island\", max_songs=80, sort=\"popularity\")\n",
        "artists.append(lonely)\n",
        "time.sleep(30)\n",
        "\n",
        "fetty = genius.search_artist(\"Fetty Wap\", max_songs=15, sort=\"popularity\")\n",
        "artists.append(fetty)\n",
        "childish = genius.search_artist(\"Childish Gambino\", max_songs=20, sort=\"popularity\")\n",
        "artists.append(childish)\n",
        "time.sleep(10)\n",
        "\n",
        "drake = genius.search_artist(\"Drake\", max_songs=20, sort=\"popularity\")\n",
        "artists.append(drake)\n",
        "kendrick = genius.search_artist(\"Kendrick Lamar\", max_songs=20, sort=\"popularity\")\n",
        "artists.append(kendrick)\n",
        "time.sleep(20)\n",
        "\n",
        "drake = genius.search_artist(\"Lil Dicky\", max_songs=20, sort=\"popularity\")\n",
        "artists.append(drake)\n",
        "chet = genius.search_artist(\"Chet Faker\", max_songs=10, sort=\"popularity\")\n",
        "artists.append(chet)\n",
        "time.sleep(30)\n",
        "\n",
        "snoop = genius.search_artist(\"Snoop Dogg\", max_songs=15, sort=\"popularity\")\n",
        "artists.append(snoop)\n",
        "travis = genius.search_artist(\"Travis Scott\", max_songs=10, sort=\"popularity\")\n",
        "artists.append(travis)\n",
        "q = genius.search_artist(\"ScHoolboy Q\", max_songs=10, sort=\"popularity\")\n",
        "artists.append(q)\n",
        "wayne = genius.search_artist(\"Lil Wayne\", max_songs=15, sort=\"popularity\")\n",
        "artists.append(wayne)\n",
        "time.sleep(30)\n",
        "\n",
        "eminem = genius.search_artist(\"Eminem\", max_songs=10, sort=\"popularity\")\n",
        "artists.append(eminem)\n",
        "ariana = genius.search_artist(\"Ariana Grande\", max_songs=5, sort=\"popularity\")\n",
        "artists.append(ariana)\n",
        "wiz = genius.search_artist(\"Wiz Khalifa\", max_songs=10, sort=\"popularity\")\n",
        "artists.append(wiz)\n",
        "beatles = genius.search_artist(\"The Beatles\", max_songs=15, sort=\"popularity\")\n",
        "artists.append(beatles)\n",
        "billie = genius.search_artist(\"Billie Eilish\", max_songs=15, sort=\"popularity\")\n",
        "artists.append(billie)\n",
        "peas = genius.search_artist(\"Black Eyed Peas\", max_songs=10, sort=\"popularity\")\n",
        "artists.append(peas)\n",
        "time.sleep(40)\n",
        "\n",
        "chance = genius.search_artist(\"Chance The Rapper\", max_songs=10, sort=\"popularity\")\n",
        "artists.append(chance)\n",
        "kanye = genius.search_artist(\"Kanye West\", max_songs=15, sort=\"popularity\")\n",
        "artists.append(kanye)\n",
        "outkast = genius.search_artist(\"OutKast\", max_songs=10, sort=\"popularity\")\n",
        "artists.append(outkast)\n",
        "rihanna = genius.search_artist(\"Rihanna\", max_songs=10, sort=\"popularity\")\n",
        "artists.append(rihanna)\n",
        "\n",
        "\n",
        "documents = []\n",
        "for artist in artists:\n",
        "  for song in artist.songs: \n",
        "    documents.append(song.lyrics)\n",
        "\n",
        "save('lyrics_array_' + fname, documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzBbUU-zdUuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COMBINE ALL SONGS INTO ONE TEXT FILE\n",
        "\n",
        "comedy_rap_text = ''\n",
        "documents = documents[81:]\n",
        "documents = documents[75:96]\n",
        "for document in documents:\n",
        "  if document is not None:\n",
        "    comedy_rap_text += document + '\\n'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpbFGKmYdt1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CLEAN THE TEXT, GET RID OF SOME PATTERNS\n",
        "\n",
        "comedy_rap_prepped = comedy_rap_text\n",
        "import re\n",
        "pattern = re.compile(r'[(.?)]')\n",
        "comedy_rap_prepped = re.sub(pattern, '', comedy_rap_prepped)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBROFg1RCZPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gpu_info = !nvidia-smi\n",
        "# gpu_info = '\\n'.join(gpu_info)\n",
        "# if gpu_info.find('failed') >= 0:\n",
        "#   print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "#   print('and then re-execute this cell.')\n",
        "# else:\n",
        "#   print(gpu_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU-klkN5O-qh",
        "colab_type": "code",
        "outputId": "e0e017bc-c7ad-4c95-8e91-c5d91724364e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# PREPROCESSING THE TRAINING DATA SO THAT CHAR MODEL CAN USE\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(comedy_rap_prepped)\n",
        "max_id = len(tokenizer.word_index) \n",
        "dataset_size = tokenizer.document_count\n",
        "\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([comedy_rap_prepped])) - 1\n",
        "\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)\n",
        "\n",
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 60) (32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B9-upMafXBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IF YOU WANT TO USE ALREADY TRAIN MODEL - rap_bot_v2\n",
        "model = keras.models.load_model('./saved_models/rap_bot_v2_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZpAqCOljvoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IF YOU WANT TO TRAIN FROM SCRATCH - RNN NETWORK WITH DROPOUT\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
        "                     dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                     dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As92lnnXTVKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6-7Fzn4f0cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO SAVE THE TRAINED NEW MODEL\n",
        "model.save('./saved_models/rap_bot_v3_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiVw2ecDHv2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HELPER FUNCTIONS FOR NEW LYRICS GENERATION\n",
        "\n",
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)\n",
        "\n",
        "def next_char(text, temperature=1):\n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model.predict(X_new)[0, -1:, :]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "\n",
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIfy092Hcn7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# THIS WILL GENERATE WARNINGS - JUST IGNORE\n",
        "# temperature param represents how the next word is sampled, close to 0 just does argmax, \n",
        "# around 1 worked good for me you can just try different and look at the results\n",
        "# to not to keep generating same things usually they dont do argmax over the probabilities instead you sample from them with the prob of the probabilities\n",
        "text = complete_text(\"c\", temperature=1, n_chars=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Y1I6bAltfa",
        "colab_type": "code",
        "outputId": "21124369-2da1-4e5d-9f0e-6c8bdf9b86bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# !!!!! LETS SEEE :D\n",
        "print(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gets fucked a mouth now\n",
            "and i'm a boat rorl like beaciena back\n",
            "this one with the sun more nice\n",
            "401k with a boat, i'm on a boat\n",
            "\n",
            "you couldn't even even  fucked out of the ass seven shawty\n",
            "\n",
            "old my bluesanimair, hey she say, i'm around me on a boat\n",
            "wear this i love the lad-elfe with the girl\n",
            "she can come to look, i'm so daddy\n",
            "i'm on that 9, let's get s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}